# HypothesisRAG

ì˜ë£Œ ë„ë©”ì¸ ì§ˆì˜ì‘ë‹µì„ ìœ„í•œ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG) íŒŒì´í”„ë¼ì¸ ì‹¤í—˜ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
í˜„ì¬ í”„ë¡œì íŠ¸ì˜ **í•µì‹¬ ìŠ¤í¬ë¦½íŠ¸ëŠ” `evaluate_medqa.py`**ì´ë©°, ì´ ì¤‘ì—ì„œë„ **`planning_v4`** ëª¨ë“œê°€ ğŸ©ºê°€ì„¤(Hypothesis)ì„ ì„¸ìš°ê³  ì´ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ RAGë¥¼ ìˆ˜í–‰í•˜ëŠ” ë³¸ í”„ë¡œì íŠ¸ì˜ **í•µì‹¬ ë°©ë²•ë¡ **ì…ë‹ˆë‹¤.

## ëª©ì 

ì—¬ëŸ¬ RAG ì „ëµì„ ë¹„êµí•˜ì—¬ ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ê°€ì„¤ì„ ì„¸ìš°ê³  ê²€ì¦í•˜ëŠ” ë°©ì‹(Hypothesis ê¸°ë°˜ Planning)ì´ ê²€ìƒ‰ í’ˆì§ˆ ë° ìµœì¢… ë‹µë³€ì— ì–´ë–¤ ë„ì›€ì„ ì£¼ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

- **(A) direct**: Query rewriting ì—†ì´ ì‚¬ìš©ì ì…ë ¥ ê·¸ëŒ€ë¡œ ê²€ìƒ‰
- **(B) baseline**: LLMì´ ìì²´ì ìœ¼ë¡œ ì¼ë°˜ì ì¸ query rewriting ìˆ˜í–‰
- **(C) planning_v4 (â­ï¸ í•µì‹¬ ë°©ë²•ë¡ )**: ì£¼ì–´ì§„ ì˜ë£Œ ë¬¸ì œì— ëŒ€í•´ ì´ˆê¸° ì§„ë‹¨ ê°€ì„¤(Best Guess)ì„ ì„¸ìš°ê³ , ì´ ê°€ì„¤ì„ í™•ì •ì§“ê±°ë‚˜ ê°ë³„ ì§„ë‹¨í•˜ê¸° ìœ„í•´ í•„ìš”í•œ êµ¬ì²´ì  ì¦ê±°ë¥¼ ì°¾ë„ë¡ íƒ€ê²ŸíŒ…ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” HypothesisRAG ë°©ë²•ë¡ 


## ì„¤ì¹˜

### 1. í™˜ê²½ ì„¤ì •

```bash
# Conda í™˜ê²½ ìƒì„± ë° í™œì„±í™” (ê¶Œì¥)
conda create -n rag_compare python=3.10
conda activate rag_compare

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
export OPENAI_API_KEY="your-openai-api-key"
```


## MedQA ë°ì´í„°ì…‹ í‰ê°€ (Async) - â­ï¸ í•µì‹¬ ìŠ¤í¬ë¦½íŠ¸ (`evaluate_medqa.py`)

`evaluate_medqa.py`ë¥¼ í†µí•´ ì „ì²´ íŒŒì´í”„ë¼ì¸ ë° HypothesisRAG(`planning_v4`)ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ ìŠ¤í¬ë¦½íŠ¸ê°€ í˜„ RAG ì‹¤í—˜ ë° ê²€ì¦ì˜ ì½”ì–´(Core) ì—­í• ì„ ìˆ˜í–‰í•˜ë©° ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ í†µí•´ ë¹ ë¥´ê²Œ ì‹¤í–‰ë©ë‹ˆë‹¤.

### í‰ê°€ ì‹¤í–‰

```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ê°œ ë¬¸ì œ)
python evaluate_medqa.py --max-questions 10

# ì „ì²´ í‰ê°€ (ëª¨ë“  ë¬¸ì œ)
# ì£¼ì˜: ëª¨ë“  ë¬¸ì œë¥¼ í‰ê°€í•˜ë ¤ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
python evaluate_medqa.py --max-questions 1273

# íŠ¹ì • ëª¨ë“œë§Œ í‰ê°€ (ì˜ˆ: í•µì‹¬ ë°©ë²•ë¡ ì¸ planning_v4 ì§‘ì¤‘ í‰ê°€)
python evaluate_medqa.py --modes direct baseline planning_v4 --max-questions 50

# Evidenceë§Œ í‰ê°€ (ë‹µë³€ ìƒì„± ì—†ìŒ - ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •ìš©)
python evaluate_medqa.py --no-answers --max-questions 100
```

### í‰ê°€ ì˜µì…˜

| ì˜µì…˜ | ì„¤ëª… | ê¸°ë³¸ê°’ |
|------|------|--------|
| `--max-questions`, `-n` | í‰ê°€í•  ìµœëŒ€ ë¬¸ì œ ìˆ˜ | 100 |
| `--modes`, `-m` | í‰ê°€í•  ëª¨ë“œë“¤ | ì „ì²´ |
| `--top-k`, `-k` | ì¿¼ë¦¬ë‹¹ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ | 5 |
| `--model` | OpenAI ëª¨ë¸ëª… | gpt-4o-mini |
| `--no-answers` | ë‹µë³€ ìƒì„± ê±´ë„ˆë›°ê¸° | False |
| `--output-dir`, `-o` | ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ | . |

## ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (`scripts/`)

- `scripts/analyze_failure_modes.py`: ì‹¤íŒ¨ ìœ í˜• ë¶„ì„ (R-GAP, R-NOISE ë“±)
- `scripts/analyze_best_guess_vs_gold.py`: Planning ë‹¨ê³„ì˜ Best Guess ì •í™•ë„ ë¶„ì„
- `scripts/compare_v4_v5_results.py`: V4 vs V5 ê²°ê³¼ ë¹„êµ

## ì£¼ìš” íŒŒì¼ êµ¬ì¡°

```text
/
â”œâ”€â”€ rag_core.py                # Core Logic (Graph definition)
â”œâ”€â”€ evaluate_medqa.py          # Main Evaluation Script (Async)
â”œâ”€â”€ run_rag.py                 # CLI Runner
â”œâ”€â”€ retriever.py               # Retriever Module
â”œâ”€â”€ config.py                  # Configuration
â”œâ”€â”€ scripts/                   # Analysis & Utility Scripts
â”‚   â”œâ”€â”€ analyze_failure_modes.py
â”‚   â”œâ”€â”€ analyze_best_guess_vs_gold.py
â”‚   â””â”€â”€ compare_v4_v5_results.py
â””â”€â”€ etc/                       # Legacy & Test Files
    â”œâ”€â”€ evaluate_medqa_sync_legacy.py # Old synchronous evaluator
    â””â”€â”€ test_json_parsing.py
```

## ì°¸ê³  ì‚¬í•­

1. **MIRAGE ê²€ìƒ‰ ì‹œìŠ¤í…œ**: ì‹¤ì œ MIRAGE MedRAG ì½”í¼ìŠ¤ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° mock retrievalì´ ì‚¬ìš©ë©ë‹ˆë‹¤.
2. **API ë¹„ìš©**: OpenAI APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Planning ëª¨ë“œëŠ” ì¶”ê°€ LLM í˜¸ì¶œì´ í•„ìš”í•©ë‹ˆë‹¤.
